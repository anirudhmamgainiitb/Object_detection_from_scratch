{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### baseline \n",
    "IMG_SIZE = 416\n",
    "GRID_SIZE = 52\n",
    "NUM_CLASSES = 7\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataloader.data_load import UnderwaterDataset  # adjust import if needed\n",
    "\n",
    "train_dataset = UnderwaterDataset(\n",
    "    img_dir=\"/Users/anirudhmamgain/Desktop/Object_detection_from_scratch/Dataset/train/images\",\n",
    "    label_dir=\"/Users/anirudhmamgain/Desktop/Object_detection_from_scratch/Dataset/train/labels\"\n",
    ")\n",
    "\n",
    "val_dataset = UnderwaterDataset(\n",
    "    img_dir=\"/Users/anirudhmamgain/Desktop/Object_detection_from_scratch/Dataset/valid/images\",\n",
    "    label_dir=\"/Users/anirudhmamgain/Desktop/Object_detection_from_scratch/Dataset/valid/labels\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anirudhmamgain/Desktop/Object_detection_from_scratch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.detector import Detector\n",
    "from models.loss import DetectionLoss\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model = Detector(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = DetectionLoss(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Total: 5.117 | Box: 0.966 | Obj: 0.163 | Cls: 2.442\n",
      "Epoch [2/50] | Total: 5.094 | Box: 0.964 | Obj: 0.059 | Cls: 4.300\n",
      "Epoch [3/50] | Total: 5.218 | Box: 0.966 | Obj: 0.057 | Cls: 6.637\n",
      "Epoch [4/50] | Total: 5.285 | Box: 0.964 | Obj: 0.055 | Cls: 8.175\n",
      "Epoch [5/50] | Total: 5.325 | Box: 0.965 | Obj: 0.050 | Cls: 9.014\n",
      "Epoch [6/50] | Total: 5.513 | Box: 0.966 | Obj: 0.065 | Cls: 12.371\n",
      "Epoch [7/50] | Total: 5.716 | Box: 0.964 | Obj: 0.054 | Cls: 16.852\n",
      "Epoch [8/50] | Total: 5.958 | Box: 0.964 | Obj: 0.058 | Cls: 21.585\n",
      "Epoch [9/50] | Total: 6.248 | Box: 0.964 | Obj: 0.062 | Cls: 27.358\n",
      "Epoch [10/50] | Total: 6.284 | Box: 0.963 | Obj: 0.050 | Cls: 28.383\n",
      "Epoch [11/50] | Total: 6.561 | Box: 0.962 | Obj: 0.044 | Cls: 34.107\n",
      "Epoch [12/50] | Total: 6.857 | Box: 0.963 | Obj: 0.061 | Cls: 39.576\n",
      "Epoch [13/50] | Total: 7.329 | Box: 0.963 | Obj: 0.041 | Cls: 49.412\n",
      "Epoch [14/50] | Total: 7.682 | Box: 0.964 | Obj: 0.058 | Cls: 56.093\n",
      "Epoch [15/50] | Total: 7.540 | Box: 0.963 | Obj: 0.063 | Cls: 53.262\n",
      "Epoch [16/50] | Total: 8.273 | Box: 0.964 | Obj: 0.076 | Cls: 67.558\n",
      "Epoch [17/50] | Total: 9.811 | Box: 0.965 | Obj: 0.049 | Cls: 98.778\n",
      "Epoch [18/50] | Total: 9.641 | Box: 0.963 | Obj: 0.032 | Cls: 95.863\n",
      "Epoch [19/50] | Total: 9.970 | Box: 0.964 | Obj: 0.054 | Cls: 101.976\n",
      "Epoch [20/50] | Total: 11.667 | Box: 0.963 | Obj: 0.054 | Cls: 135.950\n",
      "Epoch [21/50] | Total: 12.099 | Box: 0.962 | Obj: 0.067 | Cls: 144.417\n",
      "Epoch [22/50] | Total: 14.444 | Box: 0.963 | Obj: 0.062 | Cls: 191.373\n",
      "Epoch [23/50] | Total: 15.625 | Box: 0.963 | Obj: 0.060 | Cls: 214.971\n",
      "Epoch [24/50] | Total: 17.183 | Box: 0.962 | Obj: 0.063 | Cls: 246.186\n",
      "Epoch [25/50] | Total: 18.718 | Box: 0.963 | Obj: 0.061 | Cls: 276.823\n",
      "Epoch [26/50] | Total: 21.542 | Box: 0.963 | Obj: 0.054 | Cls: 333.459\n",
      "Epoch [27/50] | Total: 24.291 | Box: 0.963 | Obj: 0.044 | Cls: 388.687\n",
      "Epoch [28/50] | Total: 25.366 | Box: 0.963 | Obj: 0.049 | Cls: 410.084\n",
      "Epoch [29/50] | Total: 27.372 | Box: 0.963 | Obj: 0.043 | Cls: 450.250\n",
      "Epoch [30/50] | Total: 28.151 | Box: 0.963 | Obj: 0.047 | Cls: 465.749\n",
      "Epoch [31/50] | Total: 28.647 | Box: 0.963 | Obj: 0.050 | Cls: 475.680\n",
      "Epoch [32/50] | Total: 34.381 | Box: 0.962 | Obj: 0.043 | Cls: 590.593\n",
      "Epoch [33/50] | Total: 36.339 | Box: 0.960 | Obj: 0.047 | Cls: 629.866\n",
      "Epoch [34/50] | Total: 40.608 | Box: 0.962 | Obj: 0.039 | Cls: 715.174\n",
      "Epoch [35/50] | Total: 42.592 | Box: 0.961 | Obj: 0.043 | Cls: 754.889\n",
      "Epoch [36/50] | Total: 44.897 | Box: 0.961 | Obj: 0.040 | Cls: 801.085\n",
      "Epoch [37/50] | Total: 45.890 | Box: 0.961 | Obj: 0.041 | Cls: 820.905\n",
      "Epoch [38/50] | Total: 51.430 | Box: 0.963 | Obj: 0.039 | Cls: 931.563\n",
      "Epoch [39/50] | Total: 54.104 | Box: 0.962 | Obj: 0.039 | Cls: 985.121\n",
      "Epoch [40/50] | Total: 53.186 | Box: 0.961 | Obj: 0.038 | Cls: 966.820\n",
      "Epoch [41/50] | Total: 57.928 | Box: 0.958 | Obj: 0.036 | Cls: 1062.032\n",
      "Epoch [42/50] | Total: 63.074 | Box: 0.960 | Obj: 0.036 | Cls: 1164.744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m preds \u001b[39m=\u001b[39m model(imgs)\n\u001b[0;32m---> 22\u001b[0m box_loss, obj_loss, cls_loss \u001b[39m=\u001b[39m criterion(preds, targets)\n\u001b[1;32m     24\u001b[0m \u001b[39m# cls_loss = torch.clamp(cls_loss, max=10.0)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m5.0\u001b[39m \u001b[39m*\u001b[39m box_loss \u001b[39m+\u001b[39m obj_loss \u001b[39m+\u001b[39m \u001b[39m0.05\u001b[39m \u001b[39m*\u001b[39m cls_loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Object_detection_from_scratch/models/loss.py:79\u001b[0m, in \u001b[0;36mDetectionLoss.forward\u001b[0;34m(self, preds, targets)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m# -------- Objectness Loss --------\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m obj_mask\u001b[39m.\u001b[39many():\n\u001b[0;32m---> 79\u001b[0m     obj_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbce(\n\u001b[1;32m     80\u001b[0m         pred_obj[obj_mask],\n\u001b[1;32m     81\u001b[0m         tgt_obj[obj_mask]\n\u001b[1;32m     82\u001b[0m     )\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     83\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     obj_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m, device\u001b[39m=\u001b[39mpreds\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target,\n\u001b[1;32m    726\u001b[0m                                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[1;32m    727\u001b[0m                                               pos_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_weight,\n\u001b[1;32m    728\u001b[0m                                               reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3199\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m   3197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 3199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.makedirs(\"checkpoints/customcnn\", exist_ok=True)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    tot_loss = 0.0\n",
    "    tot_box = 0.0\n",
    "    tot_obj = 0.0\n",
    "    tot_cls = 0.0\n",
    "\n",
    "    for imgs, targets in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "\n",
    "        box_loss, obj_loss, cls_loss = criterion(preds, targets)\n",
    "\n",
    "        # cls_loss = torch.clamp(cls_loss, max=10.0)\n",
    "\n",
    "        loss = 5.0 * box_loss + obj_loss + 0.05 * cls_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tot_loss += loss.item()\n",
    "        tot_box  += box_loss.item()\n",
    "        tot_obj  += obj_loss.item()\n",
    "        tot_cls  += cls_loss.item()\n",
    "\n",
    "    avg_loss = tot_loss / len(train_loader)\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            \"checkpoints/best_model.pth\"\n",
    "        )\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        \"checkpoints/last_model.pth\"\n",
    "    )\n",
    "\n",
    "    n = len(train_loader)\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n",
    "        f\"Total: {tot_loss/n:.3f} | \"\n",
    "        f\"Box: {tot_box/n:.3f} | \"\n",
    "        f\"Obj: {tot_obj/n:.3f} | \"\n",
    "        f\"Cls: {tot_cls/n:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch [1/40] | Total: 39.936 | Box: 0.962 | Obj: 0.037 | Cls: 1169.687 | LR: 3.00e-04\n",
      "Epoch [2/40] | Total: 40.791 | Box: 0.960 | Obj: 0.035 | Cls: 1198.513 | LR: 3.00e-04\n",
      "Epoch [3/40] | Total: 41.225 | Box: 0.960 | Obj: 0.035 | Cls: 1212.920 | LR: 3.00e-04\n",
      "Epoch [4/40] | Total: 41.068 | Box: 0.959 | Obj: 0.035 | Cls: 1207.892 | LR: 3.00e-04\n",
      "Epoch [5/40] | Total: 41.666 | Box: 0.961 | Obj: 0.032 | Cls: 1227.561 | LR: 3.00e-04\n",
      "Epoch [6/40] | Total: 42.728 | Box: 0.961 | Obj: 0.033 | Cls: 1263.071 | LR: 3.00e-04\n",
      "Epoch [7/40] | Total: 41.784 | Box: 0.961 | Obj: 0.034 | Cls: 1231.546 | LR: 3.00e-04\n",
      "Epoch [8/40] | Total: 42.094 | Box: 0.961 | Obj: 0.033 | Cls: 1241.889 | LR: 3.00e-04\n",
      "Epoch [9/40] | Total: 42.414 | Box: 0.960 | Obj: 0.034 | Cls: 1252.676 | LR: 3.00e-04\n",
      "Epoch [10/40] | Total: 43.202 | Box: 0.960 | Obj: 0.032 | Cls: 1278.974 | LR: 3.00e-04\n",
      "Epoch [11/40] | Total: 43.026 | Box: 0.961 | Obj: 0.031 | Cls: 1273.045 | LR: 3.00e-04\n",
      "Epoch [12/40] | Total: 43.322 | Box: 0.961 | Obj: 0.031 | Cls: 1282.861 | LR: 3.00e-04\n",
      "Epoch [13/40] | Total: 45.249 | Box: 0.961 | Obj: 0.027 | Cls: 1347.227 | LR: 3.00e-04\n",
      "Epoch [14/40] | Total: 44.501 | Box: 0.959 | Obj: 0.029 | Cls: 1322.540 | LR: 3.00e-04\n",
      "Epoch [15/40] | Total: 44.677 | Box: 0.960 | Obj: 0.028 | Cls: 1328.293 | LR: 3.00e-05\n",
      "Epoch [16/40] | Total: 45.411 | Box: 0.960 | Obj: 0.026 | Cls: 1352.864 | LR: 3.00e-05\n",
      "Epoch [17/40] | Total: 46.570 | Box: 0.960 | Obj: 0.027 | Cls: 1391.346 | LR: 3.00e-05\n",
      "Epoch [18/40] | Total: 44.661 | Box: 0.961 | Obj: 0.029 | Cls: 1327.575 | LR: 3.00e-05\n",
      "Epoch [19/40] | Total: 46.225 | Box: 0.960 | Obj: 0.027 | Cls: 1379.896 | LR: 3.00e-05\n",
      "Epoch [20/40] | Total: 45.733 | Box: 0.961 | Obj: 0.028 | Cls: 1363.353 | LR: 3.00e-05\n",
      "Epoch [21/40] | Total: 44.858 | Box: 0.960 | Obj: 0.028 | Cls: 1334.260 | LR: 3.00e-05\n",
      "Epoch [22/40] | Total: 45.725 | Box: 0.961 | Obj: 0.028 | Cls: 1363.043 | LR: 3.00e-05\n",
      "Epoch [23/40] | Total: 44.915 | Box: 0.961 | Obj: 0.028 | Cls: 1335.971 | LR: 3.00e-05\n",
      "Epoch [24/40] | Total: 45.670 | Box: 0.960 | Obj: 0.027 | Cls: 1361.390 | LR: 3.00e-05\n",
      "Epoch [25/40] | Total: 45.250 | Box: 0.961 | Obj: 0.027 | Cls: 1347.324 | LR: 3.00e-05\n",
      "Epoch [26/40] | Total: 45.903 | Box: 0.960 | Obj: 0.027 | Cls: 1369.242 | LR: 3.00e-05\n",
      "Epoch [27/40] | Total: 45.999 | Box: 0.960 | Obj: 0.028 | Cls: 1372.478 | LR: 3.00e-05\n",
      "Epoch [28/40] | Total: 46.628 | Box: 0.960 | Obj: 0.026 | Cls: 1393.326 | LR: 3.00e-05\n",
      "Epoch [29/40] | Total: 45.890 | Box: 0.960 | Obj: 0.027 | Cls: 1368.696 | LR: 3.00e-05\n",
      "Epoch [30/40] | Total: 46.061 | Box: 0.958 | Obj: 0.028 | Cls: 1374.692 | LR: 3.00e-06\n",
      "Epoch [31/40] | Total: 45.736 | Box: 0.960 | Obj: 0.027 | Cls: 1363.562 | LR: 3.00e-06\n",
      "Epoch [32/40] | Total: 47.119 | Box: 0.960 | Obj: 0.027 | Cls: 1409.673 | LR: 3.00e-06\n",
      "Epoch [33/40] | Total: 46.103 | Box: 0.960 | Obj: 0.027 | Cls: 1375.843 | LR: 3.00e-06\n",
      "Epoch [34/40] | Total: 45.138 | Box: 0.960 | Obj: 0.027 | Cls: 1343.623 | LR: 3.00e-06\n",
      "Epoch [35/40] | Total: 46.371 | Box: 0.960 | Obj: 0.027 | Cls: 1384.834 | LR: 3.00e-06\n",
      "Epoch [36/40] | Total: 46.603 | Box: 0.959 | Obj: 0.026 | Cls: 1392.702 | LR: 3.00e-06\n",
      "Epoch [37/40] | Total: 46.891 | Box: 0.960 | Obj: 0.027 | Cls: 1402.193 | LR: 3.00e-06\n",
      "Epoch [38/40] | Total: 47.442 | Box: 0.962 | Obj: 0.027 | Cls: 1420.212 | LR: 3.00e-06\n",
      "Epoch [39/40] | Total: 46.568 | Box: 0.959 | Obj: 0.027 | Cls: 1391.509 | LR: 3.00e-06\n",
      "Epoch [40/40] | Total: 46.515 | Box: 0.961 | Obj: 0.028 | Cls: 1389.422 | LR: 3.00e-06\n"
     ]
    }
   ],
   "source": [
    "# ---------------- CONFIG ----------------\n",
    "IMG_DIR = \"/Users/anirudhmamgain/Desktop/Object_detection_from_scratch/Dataset/train/images\"\n",
    "LABEL_DIR = \"/Users/anirudhmamgain/Desktop/Object_detection_from_scratch/Dataset/train/labels\"\n",
    "CHECKPOINT = \"/Users/anirudhmamgain/Desktop/Object_detection_from_scratch/checkpoints/last_model.pth\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 40\n",
    "LR = 3e-4\n",
    "CLS_WEIGHT = 0.03\n",
    "NUM_CLASSES = 7\n",
    "FREEZE_EPOCHS = 10\n",
    "\n",
    "# ---------------- DEVICE ----------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------- DATA ----------------\n",
    "dataset = UnderwaterDataset(\n",
    "    img_dir=IMG_DIR,\n",
    "    label_dir=LABEL_DIR\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# ---------------- MODEL ----------------\n",
    "model = Detector(num_classes=NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(CHECKPOINT, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# ---------------- LOSS ----------------\n",
    "criterion = DetectionLoss(num_classes=NUM_CLASSES)\n",
    "\n",
    "# ---------------- OPTIMIZER ----------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=15,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# ---------------- TRAINING ----------------\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    # ---- Freeze backbone for first few epochs ----\n",
    "    if epoch < FREEZE_EPOCHS:\n",
    "        for p in model.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "    else:\n",
    "        for p in model.backbone.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    tot_loss = tot_box = tot_obj = tot_cls = 0.0\n",
    "\n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "\n",
    "        box_loss, obj_loss, cls_loss = criterion(preds, targets)\n",
    "        total_loss = 5.0 * box_loss + obj_loss + CLS_WEIGHT * cls_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tot_loss += total_loss.item()\n",
    "        tot_box  += box_loss.item()\n",
    "        tot_obj  += obj_loss.item()\n",
    "        tot_cls  += cls_loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    n = len(loader)\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n",
    "        f\"Total: {tot_loss/n:.3f} | \"\n",
    "        f\"Box: {tot_box/n:.3f} | \"\n",
    "        f\"Obj: {tot_obj/n:.3f} | \"\n",
    "        f\"Cls: {tot_cls/n:.3f} | \"\n",
    "        f\"LR: {scheduler.get_last_lr()[0]:.2e}\"\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), \"checkpoints/new_last_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
